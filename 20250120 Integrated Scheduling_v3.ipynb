{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-22T02:38:18.793882Z"
    }
   },
   "source": [
    "# Standard library imports\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "# Third-party imports\n",
    "import pulp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Make sure this is installed: pip install tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scheduler.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ScenarioParams:\n",
    "    \"\"\"Parameters defining a scenario\"\"\"\n",
    "    resource_levels: Dict[str, int]\n",
    "    sample_counts: Dict[str, int]\n",
    "\n",
    "@dataclass\n",
    "class ScenarioResult:\n",
    "    \"\"\"Results from running a scenario\"\"\"\n",
    "    params: ScenarioParams\n",
    "    makespan: Optional[int]\n",
    "    total_cost: float\n",
    "    samples_per_year: int\n",
    "    cost_per_sample: float\n",
    "    resource_utilization: Dict[str, float]\n",
    "    error: Optional[str] = None  # Track any errors that occurred\n",
    "\n",
    "class SchedulerError(Exception):\n",
    "    \"\"\"Custom exception for scheduler-specific errors\"\"\"\n",
    "    pass\n",
    "\n",
    "def generate_scenario_hash(scenario):\n",
    "    \"\"\"Generate a unique hash for a scenario configuration.\"\"\"\n",
    "    scenario_str = json.dumps(scenario, sort_keys=True)\n",
    "    return hashlib.md5(scenario_str.encode()).hexdigest()\n",
    "\n",
    "def load_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Load completed scenarios from checkpoint directory.\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    completed_scenarios = {}\n",
    "    \n",
    "    if checkpoint_dir.exists():\n",
    "        # Load all checkpoint files\n",
    "        for checkpoint_file in checkpoint_dir.glob(\"checkpoint_*.json\"):\n",
    "            with open(checkpoint_file, 'r') as f:\n",
    "                batch_results = json.load(f)\n",
    "                for result in batch_results:\n",
    "                    scenario_hash = generate_scenario_hash(result['scenario'])\n",
    "                    completed_scenarios[scenario_hash] = result\n",
    "    \n",
    "    return completed_scenarios\n",
    "\n",
    "def save_checkpoint(results_batch, checkpoint_dir, batch_num):\n",
    "    \"\"\"Save a batch of results to a checkpoint file.\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    checkpoint_file = checkpoint_dir / f\"checkpoint_{batch_num:04d}.json\"\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(results_batch, f, indent=2)\n",
    "\n",
    "def visualize_results(results_df):\n",
    "    \"\"\"Create enhanced visualizations for scheduling results.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Resource Utilization Plot\n",
    "    ax1 = plt.subplot(321)\n",
    "    utilization_cols = [col for col in results_df.columns if col.startswith('utilization_')]\n",
    "    utilization_data = results_df[utilization_cols].mean()\n",
    "    utilization_data = utilization_data.sort_values(ascending=True)\n",
    "    \n",
    "    sns.barplot(x=utilization_data.values * 100, \n",
    "                y=[col.replace('utilization_', '').replace('_', ' ') for col in utilization_data.index],\n",
    "                ax=ax1)\n",
    "    ax1.set_title('Average Resource Utilization (%)')\n",
    "    ax1.set_xlabel('Utilization %')\n",
    "    \n",
    "    # 2. Capacity vs Throughput Analysis\n",
    "    ax2 = plt.subplot(322)\n",
    "    total_resources = results_df[[col for col in results_df.columns if col.startswith('resource_')]].sum(axis=1)\n",
    "    sns.scatterplot(data=results_df, \n",
    "                    x='samples_per_year', \n",
    "                    y='total_cost',\n",
    "                    hue=total_resources,\n",
    "                    size=total_resources,\n",
    "                    alpha=0.6,\n",
    "                    ax=ax2)\n",
    "    ax2.set_title('Capacity vs Throughput/Cost Tradeoff')\n",
    "    ax2.set_xlabel('Annual Throughput (samples/year)')\n",
    "    ax2.set_ylabel('Total Cost ($)')\n",
    "    \n",
    "    # 3. Resource Allocation Impact\n",
    "    ax3 = plt.subplot(323)\n",
    "    resource_cols = [col for col in results_df.columns if col.startswith('resource_')]\n",
    "    for col in resource_cols:\n",
    "        sns.kdeplot(data=results_df[results_df[col] > 0], \n",
    "                   x='samples_per_year',\n",
    "                   hue=col,\n",
    "                   ax=ax3)\n",
    "    ax3.set_title('Impact of Resource Levels on Throughput')\n",
    "    ax3.set_xlabel('Annual Throughput (samples/year)')\n",
    "    \n",
    "    # 4. Cost Efficiency Analysis\n",
    "    ax4 = plt.subplot(324)\n",
    "    sns.scatterplot(data=results_df,\n",
    "                    x='samples_per_year',\n",
    "                    y='cost_per_sample',\n",
    "                    hue=total_resources,\n",
    "                    size=total_resources,\n",
    "                    alpha=0.6,\n",
    "                    ax=ax4)\n",
    "    ax4.set_title('Cost Efficiency Analysis')\n",
    "    ax4.set_xlabel('Annual Throughput (samples/year)')\n",
    "    ax4.set_ylabel('Cost per Sample ($)')\n",
    "    \n",
    "    # 5. Sample Mix Analysis\n",
    "    ax5 = plt.subplot(325)\n",
    "    sample_cols = [col for col in results_df.columns if col.startswith('samples_')]\n",
    "    sample_data = results_df[sample_cols].mean()\n",
    "    sns.barplot(x=[col.replace('samples_', '').replace('_', ' ') for col in sample_cols],\n",
    "                y=sample_data.values,\n",
    "                ax=ax5)\n",
    "    ax5.set_title('Average Sample Mix')\n",
    "    ax5.set_ylabel('Number of Samples')\n",
    "    \n",
    "    # 6. Makespan Distribution\n",
    "    ax6 = plt.subplot(326)\n",
    "    sns.histplot(data=results_df, x='makespan', bins=30, ax=ax6)\n",
    "    ax6.set_title('Distribution of Makespan')\n",
    "    ax6.set_xlabel('Makespan (days)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def create_gantt_chart(tasks, start_times, resource_capacities, makespan):\n",
    "    \"\"\"\n",
    "    Create a Gantt chart visualization of the schedule.\n",
    "    \n",
    "    Args:\n",
    "        tasks: List of task dictionaries\n",
    "        start_times: Dictionary of task start times\n",
    "        resource_capacities: Dictionary of resource capacities\n",
    "        makespan: Total schedule duration\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    \n",
    "    # Prepare data\n",
    "    task_data = []\n",
    "    for task in tasks:\n",
    "        task_id = task['id']\n",
    "        if task_id in start_times:\n",
    "            task_data.append({\n",
    "                'Task': task_id,\n",
    "                'Resource': task['resource'],\n",
    "                'Start': start_times[task_id],\n",
    "                'Duration': task['duration'],\n",
    "                'End': start_times[task_id] + task['duration']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(task_data)\n",
    "    \n",
    "    # Sort by resource and start time\n",
    "    df = df.sort_values(['Resource', 'Start'])\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), height_ratios=[4, 1])\n",
    "    \n",
    "    # Plot tasks\n",
    "    resources = df['Resource'].unique()\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(resources)))\n",
    "    resource_colors = dict(zip(resources, colors))\n",
    "    \n",
    "    y_labels = []\n",
    "    y_ticks = []\n",
    "    current_y = 0\n",
    "    \n",
    "    for i, (resource, group) in enumerate(df.groupby('Resource')):\n",
    "        tasks_in_resource = len(group)\n",
    "        y_positions = np.arange(current_y, current_y + tasks_in_resource)\n",
    "        \n",
    "        for idx, task in group.iterrows():\n",
    "            ax1.barh(y_positions[len(y_labels)], \n",
    "                    task['Duration'],\n",
    "                    left=task['Start'],\n",
    "                    color=resource_colors[resource],\n",
    "                    alpha=0.8)\n",
    "            \n",
    "            # Add task label\n",
    "            ax1.text(task['Start'], y_positions[len(y_labels)], \n",
    "                    f\" {task['Task']}\", \n",
    "                    va='center', fontsize=8)\n",
    "            \n",
    "            y_labels.append(f\"{task['Task']}\")\n",
    "            y_ticks.append(y_positions[len(y_labels)-1])\n",
    "        \n",
    "        current_y += tasks_in_resource + 1\n",
    "\n",
    "    # Customize first subplot\n",
    "    ax1.set_ylim(-1, current_y)\n",
    "    ax1.set_xlim(-1, makespan + 1)\n",
    "    ax1.set_yticks(y_ticks)\n",
    "    ax1.set_yticklabels(y_labels)\n",
    "    ax1.grid(True, axis='x', alpha=0.3)\n",
    "    ax1.set_title('Task Schedule by Resource', pad=20)\n",
    "    ax1.set_xlabel('Time (days)')\n",
    "    \n",
    "    # Add resource utilization subplot\n",
    "    time_points = np.arange(makespan + 1)\n",
    "    for resource, color in resource_colors.items():\n",
    "        utilization = []\n",
    "        for t in time_points:\n",
    "            active_tasks = len([\n",
    "                task for task in task_data \n",
    "                if task['Resource'] == resource \n",
    "                and task['Start'] <= t < task['End']\n",
    "            ])\n",
    "            utilization.append(active_tasks / resource_capacities[resource])\n",
    "        \n",
    "        ax2.plot(time_points, utilization, \n",
    "                label=resource, color=color, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlim(-1, makespan + 1)\n",
    "    ax2.set_ylim(0, 1.5)  # Allow some overflow to show over-utilization\n",
    "    ax2.set_xlabel('Time (days)')\n",
    "    ax2.set_ylabel('Resource Utilization')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def create_pareto_frontier(results_df):\n",
    "    \"\"\"\n",
    "    Create Pareto frontier analysis for throughput vs cost tradeoffs.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame containing scenario results\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # 1. Throughput vs Cost Frontier\n",
    "    points = results_df[['samples_per_year', 'total_cost']].values\n",
    "    \n",
    "    # Identify Pareto frontier points\n",
    "    is_pareto = np.ones(len(points), dtype=bool)\n",
    "    for i, point in enumerate(points):\n",
    "        if is_pareto[i]:\n",
    "            # Keep points with either lower cost or higher throughput\n",
    "            is_pareto[is_pareto] = np.any(points[is_pareto] < point, axis=1) | np.all(points[is_pareto] == point, axis=1)\n",
    "            is_pareto[i] = True  # Keep current point\n",
    "    \n",
    "    pareto_points = points[is_pareto]\n",
    "    sorted_pareto = pareto_points[np.argsort(pareto_points[:, 0])]\n",
    "    \n",
    "    # Plot all points\n",
    "    scatter = ax1.scatter(results_df['samples_per_year'], \n",
    "                         results_df['total_cost'],\n",
    "                         c=results_df['resource_count'], \n",
    "                         cmap='viridis',\n",
    "                         alpha=0.6)\n",
    "    \n",
    "    # Plot Pareto frontier\n",
    "    ax1.plot(sorted_pareto[:, 0], sorted_pareto[:, 1], \n",
    "             'r--', linewidth=2, label='Pareto Frontier')\n",
    "    \n",
    "    ax1.set_xlabel('Annual Throughput (samples/year)')\n",
    "    ax1.set_ylabel('Total Cost ($)')\n",
    "    ax1.set_title('Cost-Throughput Pareto Frontier')\n",
    "    ax1.legend()\n",
    "    plt.colorbar(scatter, ax=ax1, label='Total Resources')\n",
    "    \n",
    "    # 2. Resource Efficiency Frontier\n",
    "    results_df['cost_per_sample'] = results_df['total_cost'] / results_df['samples_per_year']\n",
    "    points_eff = results_df[['samples_per_year', 'cost_per_sample']].values\n",
    "    \n",
    "    # Identify efficiency frontier\n",
    "    is_efficient = np.ones(len(points_eff), dtype=bool)\n",
    "    for i, point in enumerate(points_eff):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = np.any(points_eff[is_efficient] < point, axis=1) | np.all(points_eff[is_efficient] == point, axis=1)\n",
    "            is_efficient[i] = True\n",
    "    \n",
    "    efficient_points = points_eff[is_efficient]\n",
    "    sorted_efficient = efficient_points[np.argsort(efficient_points[:, 0])]\n",
    "    \n",
    "    # Plot efficiency points\n",
    "    scatter2 = ax2.scatter(results_df['samples_per_year'],\n",
    "                          results_df['cost_per_sample'],\n",
    "                          c=results_df['resource_count'],\n",
    "                          cmap='viridis',\n",
    "                          alpha=0.6)\n",
    "    \n",
    "    # Plot efficiency frontier\n",
    "    ax2.plot(sorted_efficient[:, 0], sorted_efficient[:, 1],\n",
    "             'r--', linewidth=2, label='Efficiency Frontier')\n",
    "    \n",
    "    ax2.set_xlabel('Annual Throughput (samples/year)')\n",
    "    ax2.set_ylabel('Cost per Sample ($)')\n",
    "    ax2.set_title('Cost Efficiency Frontier')\n",
    "    ax2.legend()\n",
    "    plt.colorbar(scatter2, ax=ax2, label='Total Resources')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_tradeoffs(results_df):\n",
    "    \"\"\"Analyze key performance tradeoffs and efficiency frontiers.\"\"\"\n",
    "    # Calculate efficiency metrics\n",
    "    results_df['resource_count'] = results_df[[col for col in results_df.columns \n",
    "                                             if col.startswith('resource_')]].sum(axis=1)\n",
    "    results_df['throughput_per_resource'] = results_df['samples_per_year'] / results_df['resource_count']\n",
    "    results_df['cost_per_throughput'] = results_df['total_cost'] / results_df['samples_per_year']\n",
    "    \n",
    "    # Create efficiency frontier visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    \n",
    "    # 1. Throughput vs Resources\n",
    "    sns.scatterplot(data=results_df,\n",
    "                    x='resource_count',\n",
    "                    y='samples_per_year',\n",
    "                    ax=axes[0,0])\n",
    "    axes[0,0].set_title('Throughput vs Total Resources')\n",
    "    \n",
    "    # 2. Cost Efficiency\n",
    "    sns.scatterplot(data=results_df,\n",
    "                    x='samples_per_year',\n",
    "                    y='cost_per_throughput',\n",
    "                    ax=axes[0,1])\n",
    "    axes[0,1].set_title('Cost Efficiency vs Throughput')\n",
    "    \n",
    "    # 3. Resource Utilization vs Throughput\n",
    "    util_cols = [col for col in results_df.columns if col.startswith('utilization_')]\n",
    "    results_df['avg_utilization'] = results_df[util_cols].mean(axis=1)\n",
    "    sns.scatterplot(data=results_df,\n",
    "                    x='samples_per_year',\n",
    "                    y='avg_utilization',\n",
    "                    ax=axes[1,0])\n",
    "    axes[1,0].set_title('Resource Utilization vs Throughput')\n",
    "    \n",
    "    # 4. Throughput per Resource\n",
    "    sns.scatterplot(data=results_df,\n",
    "                    x='resource_count',\n",
    "                    y='throughput_per_resource',\n",
    "                    ax=axes[1,1])\n",
    "    axes[1,1].set_title('Efficiency (Throughput per Resource)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "# Example usage in your notebook:\n",
    "def analyze_results(output_file):\n",
    "    \"\"\"Load and analyze results with visualizations.\"\"\"\n",
    "    # Read the results\n",
    "    results_df = pd.read_csv(output_file)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(f\"Average makespan: {results_df['makespan'].mean():.1f} days\")\n",
    "    print(f\"Average cost: ${results_df['total_cost'].mean():.2f}\")\n",
    "    print(f\"Average samples per year: {results_df['samples_per_year'].mean():.1f}\")\n",
    "    \n",
    "    # Show resource utilization\n",
    "    utilization_cols = [col for col in results_df.columns if col.startswith('utilization_')]\n",
    "    print(\"\\nResource Utilization:\")\n",
    "    for col in utilization_cols:\n",
    "        resource = col.replace('utilization_', '')\n",
    "        util = results_df[col].mean() * 100\n",
    "        print(f\"{resource}: {util:.1f}%\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig = visualize_results(results_df)\n",
    "    return results_df, fig\n",
    "\n",
    "def prepare_dashboard_data(results):\n",
    "    \"\"\"Process results into a format needed for dashboard visualization.\"\"\"\n",
    "    # Initialize summary structures\n",
    "    summary = defaultdict(lambda: {\"total_cost\": 0, \"samples_per_year\": 0, \"scenarios\": 0})\n",
    "    mix = defaultdict(lambda: defaultdict(int))\n",
    "    timelines = []\n",
    "    sample_types = ['metals', 'ceramics', 'composites', 'polymers']\n",
    "    sample_summaries = {stype: [] for stype in sample_types}\n",
    "\n",
    "    # Process each result\n",
    "    for result in results:\n",
    "        # Skip if no makespan (indicating failed scenario)\n",
    "        if result.get('makespan') is None:\n",
    "            continue\n",
    "\n",
    "        scenario = result['scenario']\n",
    "        total_resources = sum(scenario['resource_capacities'].values())\n",
    "        \n",
    "        # Calculate total samples\n",
    "        total_samples = (\n",
    "            scenario.get('metals_count', 0) +\n",
    "            scenario.get('ceramics_count', 0) +\n",
    "            scenario.get('composites_count', 0) +\n",
    "            scenario.get('polymer_count', 0)\n",
    "        )\n",
    "        \n",
    "        if total_samples == 0:\n",
    "            continue\n",
    "\n",
    "        # Summary data\n",
    "        summary_key = total_resources\n",
    "        summary[summary_key][\"total_cost\"] += result['total_cost']\n",
    "        summary[summary_key][\"samples_per_year\"] += result.get('samples_per_year', 0)\n",
    "        summary[summary_key][\"scenarios\"] += 1\n",
    "\n",
    "        # Mix data\n",
    "        mix[summary_key]['metals'] += scenario.get('metals_count', 0)\n",
    "        mix[summary_key]['ceramics'] += scenario.get('ceramics_count', 0)\n",
    "        mix[summary_key]['composites'] += scenario.get('composites_count', 0)\n",
    "        mix[summary_key]['polymers'] += scenario.get('polymer_count', 0)\n",
    "\n",
    "        # Timeline data\n",
    "        timelines.append({\n",
    "            \"scenario\": {k: v for k, v in scenario.items()},  # Create a copy\n",
    "            \"makespan\": result['makespan'],\n",
    "            \"total_cost\": result['total_cost']\n",
    "        })\n",
    "\n",
    "        # Sample summaries\n",
    "        annual_factor = 365 / result['makespan']\n",
    "        for sample_type in sample_types:\n",
    "            count = scenario.get(f\"{sample_type}_count\", 0)\n",
    "            if count > 0:\n",
    "                annual_samples = int(count * annual_factor)\n",
    "                type_cost = result['total_cost'] * (count / total_samples)\n",
    "                cost_per_sample = type_cost / count\n",
    "\n",
    "                sample_summaries[sample_type].append({\n",
    "                    'total_resources': total_resources,\n",
    "                    'samples_per_year': annual_samples,\n",
    "                    'cost_per_sample': round(cost_per_sample, 2)\n",
    "                })\n",
    "\n",
    "    # Create final dashboard data structure\n",
    "    dashboard_data = {\n",
    "        'summary': [\n",
    "            {\n",
    "                \"total_resources\": key,\n",
    "                \"average_cost\": value[\"total_cost\"] / value[\"scenarios\"],\n",
    "                \"average_samples_per_year\": value[\"samples_per_year\"] / value[\"scenarios\"]\n",
    "            }\n",
    "            for key, value in summary.items()\n",
    "        ],\n",
    "        'mix': [\n",
    "            {\n",
    "                \"total_resources\": key,\n",
    "                **values\n",
    "            }\n",
    "            for key, values in mix.items()\n",
    "        ],\n",
    "        'timelines': timelines,\n",
    "        'sample_summaries': sample_summaries\n",
    "    }\n",
    "\n",
    "    # Save to JSON\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dashboard_file = f\"dashboard_data_{timestamp}.json\"\n",
    "    \n",
    "    with open(dashboard_file, 'w') as f:\n",
    "        json.dump(dashboard_data, f, indent=4)\n",
    "    \n",
    "    logging.info(f\"Dashboard data prepared and saved to '{dashboard_file}'\")\n",
    "    return dashboard_data\n",
    "\n",
    "\n",
    "\n",
    "def validate_scenario(scenario: dict, templates: List[dict]) -> None:\n",
    "    \"\"\"Validate that a scenario has necessary resources for the templates.\"\"\"\n",
    "    required_resources = set()\n",
    "    for template in templates:\n",
    "        for task in template:\n",
    "            required_resources.add(task[\"resource\"])\n",
    "    \n",
    "    available_resources = set(scenario['resource_capacities'].keys())\n",
    "    missing_resources = required_resources - available_resources\n",
    "    \n",
    "    if missing_resources:\n",
    "        raise SchedulerError(\n",
    "            f\"Missing required resources: {missing_resources}. \"\n",
    "            \"These resources are needed for tasks in the templates.\"\n",
    "        )\n",
    "\n",
    "def run_scenarios_with_checkpoints(\n",
    "    scenarios, \n",
    "    templates, \n",
    "    resource_unit_costs, \n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "    checkpoint_frequency=100,  # Save every N scenarios\n",
    "    max_workers=None\n",
    "):\n",
    "    \"\"\"Run scenarios with checkpointing support.\n",
    "    \n",
    "    Args:\n",
    "        scenarios: List of scenarios to evaluate\n",
    "        templates: List of WBS templates\n",
    "        resource_unit_costs: Dictionary of resource costs\n",
    "        checkpoint_dir: Directory for checkpoint files\n",
    "        checkpoint_frequency: How often to save checkpoints\n",
    "        max_workers: Number of worker processes\n",
    "    \"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "    \n",
    "    # Load existing checkpoints\n",
    "    completed_scenarios = load_checkpoint(checkpoint_dir)\n",
    "    logging.info(f\"Loaded {len(completed_scenarios)} completed scenarios from checkpoints\")\n",
    "    \n",
    "    # Filter out completed scenarios\n",
    "    scenarios_to_run = []\n",
    "    results = []\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        scenario_hash = generate_scenario_hash(scenario)\n",
    "        if scenario_hash in completed_scenarios:\n",
    "            results.append(completed_scenarios[scenario_hash])\n",
    "        else:\n",
    "            scenarios_to_run.append(scenario)\n",
    "    \n",
    "    total_scenarios = len(scenarios_to_run)\n",
    "    if total_scenarios == 0:\n",
    "        logging.info(\"All scenarios already completed!\")\n",
    "        return results\n",
    "    \n",
    "    logging.info(f\"Running {total_scenarios} remaining scenarios using {max_workers} processes\")\n",
    "    completed = 0\n",
    "    current_batch = []\n",
    "    current_batch_num = len(completed_scenarios) // checkpoint_frequency\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit remaining scenarios\n",
    "        future_to_scenario = {\n",
    "            executor.submit(\n",
    "                process_scenario, \n",
    "                scenario, \n",
    "                templates, \n",
    "                resource_unit_costs,\n",
    "                i + 1,\n",
    "                total_scenarios\n",
    "            ): i for i, scenario in enumerate(scenarios_to_run)\n",
    "        }\n",
    "        \n",
    "        # Process with progress bar\n",
    "        with tqdm(total=total_scenarios, desc=\"Processing scenarios\") as pbar:\n",
    "            for future in as_completed(future_to_scenario):\n",
    "                scenario_idx = future_to_scenario[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    current_batch.append(result)\n",
    "                    completed += 1\n",
    "                    \n",
    "                    # Save checkpoint if needed\n",
    "                    if len(current_batch) >= checkpoint_frequency:\n",
    "                        save_checkpoint(current_batch, checkpoint_dir, current_batch_num)\n",
    "                        current_batch_num += 1\n",
    "                        current_batch = []\n",
    "                        \n",
    "                        # Also save a CSV summary\n",
    "                        summary_df = pd.DataFrame(results)\n",
    "                        summary_df.to_csv(Path(checkpoint_dir) / \"results_summary.csv\", index=False)\n",
    "                    \n",
    "                    # Progress updates\n",
    "                    if completed % max(1, total_scenarios // 100) == 0 or result.get('error'):\n",
    "                        elapsed = time.time() - start_time\n",
    "                        rate = completed / elapsed\n",
    "                        eta = (total_scenarios - completed) / rate if rate > 0 else 0\n",
    "                        logging.info(\n",
    "                            f\"\\nProgress Update:\\n\"\n",
    "                            f\"Completed {completed}/{total_scenarios} scenarios \"\n",
    "                            f\"({completed/total_scenarios*100:.1f}%)\\n\"\n",
    "                            f\"Rate: {rate:.1f} scenarios/sec\\n\"\n",
    "                            f\"ETA: {eta/60:.1f} minutes\\n\"\n",
    "                            f\"Last checkpoint: Batch {current_batch_num}\"\n",
    "                        )\n",
    "                        \n",
    "                        if result.get('error'):\n",
    "                            logging.error(\n",
    "                                f\"Scenario {scenario_idx + 1} failed: {result.get('error')}\\n\"\n",
    "                                f\"Configuration: {scenarios_to_run[scenario_idx]}\"\n",
    "                            )\n",
    "                        elif result.get('makespan'):\n",
    "                            logging.info(\n",
    "                                f\"Scenario {scenario_idx + 1} succeeded:\\n\"\n",
    "                                f\"Makespan: {result['makespan']}\\n\"\n",
    "                                f\"Total Cost: {result['total_cost']:.2f}\"\n",
    "                            )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing scenario {scenario_idx + 1}: {str(e)}\")\n",
    "                    error_result = {\n",
    "                        'scenario': scenarios_to_run[scenario_idx],\n",
    "                        'makespan': None,\n",
    "                        'total_cost': 0,\n",
    "                        'usage': {},\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "                    results.append(error_result)\n",
    "                    current_batch.append(error_result)\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Save final batch\n",
    "    if current_batch:\n",
    "        save_checkpoint(current_batch, checkpoint_dir, current_batch_num)\n",
    "        summary_df = pd.DataFrame(results)\n",
    "        summary_df.to_csv(Path(checkpoint_dir) / \"results_summary.csv\", index=False)\n",
    "    \n",
    "    # Final summary\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    successful = sum(1 for r in results if r.get('makespan') is not None)\n",
    "    \n",
    "    logging.info(\n",
    "        f\"\\nFinal Results:\\n\"\n",
    "        f\"Total scenarios: {len(results)}\\n\"\n",
    "        f\"Successfully completed: {successful}/{len(results)} \"\n",
    "        f\"({successful/len(results)*100:.1f}%)\\n\"\n",
    "        f\"Failed: {len(results)-successful}/{len(results)} \"\n",
    "        f\"({(len(results)-successful)/len(results)*100:.1f}%)\\n\"\n",
    "        f\"Total runtime: {total_time/3600:.1f} hours\\n\"\n",
    "        f\"Average rate: {total_scenarios/total_time:.1f} scenarios/sec\\n\"\n",
    "        f\"Results saved in: {checkpoint_dir}\"\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_scenarios_optimized(scenarios, templates, resource_unit_costs, max_workers=None):\n",
    "    \"\"\"Run all scenarios in parallel with process-based parallelization.\n",
    "    \n",
    "    Args:\n",
    "        scenarios: List of scenarios to evaluate\n",
    "        templates: List of WBS templates\n",
    "        resource_unit_costs: Dictionary of resource costs\n",
    "        max_workers: Number of worker processes (defaults to CPU count - 1)\n",
    "    \"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "    \n",
    "    total_scenarios = len(scenarios)\n",
    "    completed = 0\n",
    "    results = []\n",
    "    \n",
    "    logging.info(f\"Starting optimization of {total_scenarios} scenarios using {max_workers} processes\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all scenarios\n",
    "        future_to_scenario = {\n",
    "            executor.submit(\n",
    "                process_scenario, \n",
    "                scenario, \n",
    "                templates, \n",
    "                resource_unit_costs,\n",
    "                i + 1,\n",
    "                total_scenarios\n",
    "            ): i for i, scenario in enumerate(scenarios)\n",
    "        }\n",
    "        \n",
    "        # Process completed scenarios with progress bar\n",
    "        with tqdm(total=total_scenarios, desc=\"Processing scenarios\") as pbar:\n",
    "            for future in as_completed(future_to_scenario):\n",
    "                scenario_idx = future_to_scenario[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    completed += 1\n",
    "                    \n",
    "                    # Update progress every N scenarios or when there's an error\n",
    "                    if completed % max(1, total_scenarios // 100) == 0 or result.get('error'):\n",
    "                        elapsed = time.time() - start_time\n",
    "                        rate = completed / elapsed\n",
    "                        eta = (total_scenarios - completed) / rate if rate > 0 else 0\n",
    "                        logging.info(\n",
    "                            f\"\\nProgress Update:\\n\"\n",
    "                            f\"Completed {completed}/{total_scenarios} scenarios \"\n",
    "                            f\"({completed/total_scenarios*100:.1f}%)\\n\"\n",
    "                            f\"Rate: {rate:.1f} scenarios/sec\\n\"\n",
    "                            f\"ETA: {eta/60:.1f} minutes\"\n",
    "                        )\n",
    "                        \n",
    "                        if result.get('error'):\n",
    "                            logging.error(\n",
    "                                f\"Scenario {scenario_idx + 1} failed: {result.get('error')}\\n\"\n",
    "                                f\"Configuration: {scenarios[scenario_idx]}\"\n",
    "                            )\n",
    "                        elif result.get('makespan'):\n",
    "                            logging.info(\n",
    "                                f\"Scenario {scenario_idx + 1} succeeded:\\n\"\n",
    "                                f\"Makespan: {result['makespan']}\\n\"\n",
    "                                f\"Total Cost: {result['total_cost']:.2f}\"\n",
    "                            )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing scenario {scenario_idx + 1}: {str(e)}\")\n",
    "                    results.append({\n",
    "                        'scenario': scenarios[scenario_idx],\n",
    "                        'makespan': None,\n",
    "                        'total_cost': 0,\n",
    "                        'usage': {},\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Final summary\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    successful = sum(1 for r in results if r.get('makespan') is not None)\n",
    "    \n",
    "    logging.info(\n",
    "        f\"\\nCompleted {total_scenarios} scenarios in {total_time:.1f} seconds\\n\"\n",
    "        f\"Successful: {successful}/{total_scenarios} ({successful/total_scenarios*100:.1f}%)\\n\"\n",
    "        f\"Failed: {total_scenarios-successful}/{total_scenarios} \"\n",
    "        f\"({(total_scenarios-successful)/total_scenarios*100:.1f}%)\\n\"\n",
    "        f\"Average rate: {total_scenarios/total_time:.1f} scenarios/sec\"\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_scenarios(\n",
    "    resource_ranges: Dict[str, Tuple[int, int]],  # e.g., {\"Admin\": (1,3)}\n",
    "    sample_ranges: Dict[str, Tuple[int, int]],    # e.g., {\"metals\": (0,10)}\n",
    "    step_sizes: Dict[str, int] = None             # Optional step sizes\n",
    ") -> List[ScenarioParams]:\n",
    "    \"\"\"Generate all scenario combinations within given ranges\"\"\"\n",
    "    if step_sizes is None:\n",
    "        step_sizes = {k: 1 for k in {**resource_ranges, **sample_ranges}.keys()}\n",
    "    \n",
    "    # Generate resource level combinations\n",
    "    resource_values = [\n",
    "        range(start, end + 1, step_sizes.get(resource, 1))\n",
    "        for resource, (start, end) in resource_ranges.items()\n",
    "    ]\n",
    "    resource_combinations = list(itertools.product(*resource_values))\n",
    "    \n",
    "    # Generate sample count combinations\n",
    "    sample_values = [\n",
    "        range(start, end + 1, step_sizes.get(sample_type, 1))\n",
    "        for sample_type, (start, end) in sample_ranges.items()\n",
    "    ]\n",
    "    sample_combinations = list(itertools.product(*sample_values))\n",
    "    \n",
    "    # Create all possible combinations\n",
    "    scenarios = []\n",
    "    for res_combo in resource_combinations:\n",
    "        for sample_combo in sample_combinations:\n",
    "            resource_dict = dict(zip(resource_ranges.keys(), res_combo))\n",
    "            sample_dict = dict(zip(sample_ranges.keys(), sample_combo))\n",
    "            \n",
    "            # Validate scenario parameters\n",
    "            if all(count == 0 for count in sample_dict.values()):\n",
    "                logging.warning(f\"Skipping invalid scenario with no samples: {sample_dict}\")\n",
    "                continue\n",
    "            \n",
    "            scenarios.append(ScenarioParams(\n",
    "                resource_levels=resource_dict,\n",
    "                sample_counts=sample_dict\n",
    "            ))\n",
    "    return scenarios\n",
    "\n",
    "############################\n",
    "# Critical Path Calculation\n",
    "############################\n",
    "def compute_earliest_starts(tasks):\n",
    "    \"\"\"Compute earliest possible start times using critical path method.\"\"\"\n",
    "    task_dict = {t[\"id\"]: t for t in tasks}\n",
    "    earliest_starts = {}\n",
    "    \n",
    "    def get_earliest_start(task_id, memo=None):\n",
    "        if memo is None:\n",
    "            memo = {}\n",
    "        if task_id in memo:\n",
    "            return memo[task_id]\n",
    "            \n",
    "        task = task_dict[task_id]\n",
    "        if not task[\"dependencies\"]:\n",
    "            memo[task_id] = 0\n",
    "            return 0\n",
    "            \n",
    "        max_pred_finish = 0\n",
    "        for pred_id in task[\"dependencies\"]:\n",
    "            pred_start = get_earliest_start(pred_id, memo)\n",
    "            pred_finish = pred_start + task_dict[pred_id][\"duration\"]\n",
    "            max_pred_finish = max(max_pred_finish, pred_finish)\n",
    "            \n",
    "        memo[task_id] = max_pred_finish\n",
    "        return max_pred_finish\n",
    "    \n",
    "    for task in tasks:\n",
    "        earliest_starts[task[\"id\"]] = get_earliest_start(task[\"id\"])\n",
    "        \n",
    "    return earliest_starts\n",
    "\n",
    "############################\n",
    "# Resource Usage and Cost\n",
    "############################\n",
    "def compute_resource_usage_cost(tasks, start_times, resource_unit_costs, makespan):\n",
    "    \"\"\"Calculate resource usage and cost over the makespan.\"\"\"\n",
    "    resource_usage = defaultdict(lambda: defaultdict(int))\n",
    "    for task_id, start_time in start_times.items():\n",
    "        task = next(t for t in tasks if t[\"id\"] == task_id)\n",
    "        for t in range(start_time, start_time + task[\"duration\"]):\n",
    "            if t <= makespan:\n",
    "                resource_usage[task[\"resource\"]][t] += 1\n",
    "    \n",
    "    total_cost = sum(\n",
    "        usage * resource_unit_costs.get(r, 0.0)\n",
    "        for r, times in resource_usage.items()\n",
    "        for usage in times.values()\n",
    "    )\n",
    "    return total_cost, dict(resource_usage)\n",
    "\n",
    "############################\n",
    "# Run Scenarios\n",
    "############################\n",
    "\n",
    "def process_and_save_results(scenarios: List[ScenarioParams], \n",
    "                           raw_results: List[dict], \n",
    "                           output_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process raw scheduling results into final format and save to CSV.\"\"\"\n",
    "    final_results = []\n",
    "    \n",
    "    for scenario, result in zip(scenarios, raw_results):\n",
    "        # Add error handling for successful cases\n",
    "        if result.get('makespan') is not None:  # Changed from error check to makespan check\n",
    "            # Calculate samples per year (assuming makespan is in days)\n",
    "            total_samples = sum(scenario.sample_counts.values())\n",
    "            samples_per_year = int(365 * total_samples / result['makespan'])\n",
    "            \n",
    "            # Calculate cost per sample\n",
    "            cost_per_sample = result['total_cost'] / total_samples if total_samples > 0 else 0\n",
    "            \n",
    "            # Calculate resource utilization\n",
    "            utilization = {}\n",
    "            for resource, usage_dict in result['usage'].items():\n",
    "                max_possible_usage = scenario.resource_levels[resource] * result['makespan']\n",
    "                total_usage = sum(usage_dict.values())\n",
    "                utilization[resource] = total_usage / max_possible_usage if max_possible_usage > 0 else 0\n",
    "            \n",
    "            final_results.append(ScenarioResult(\n",
    "                params=scenario,\n",
    "                makespan=result['makespan'],\n",
    "                total_cost=result['total_cost'],\n",
    "                samples_per_year=samples_per_year,\n",
    "                cost_per_sample=cost_per_sample,\n",
    "                resource_utilization=utilization,\n",
    "                error=None\n",
    "            ))\n",
    "        else:\n",
    "            # Handle failed scenarios\n",
    "            final_results.append(ScenarioResult(\n",
    "                params=scenario,\n",
    "                makespan=None,\n",
    "                total_cost=0.0,\n",
    "                samples_per_year=0,\n",
    "                cost_per_sample=0.0,\n",
    "                resource_utilization={},\n",
    "                error=result.get('error', 'Unknown error')\n",
    "            ))\n",
    "    \n",
    "    # Convert to DataFrame for saving\n",
    "    records = []\n",
    "    for result in final_results:\n",
    "        record = {\n",
    "            # Resource levels\n",
    "            **{f\"resource_{k}\": v for k, v in result.params.resource_levels.items()},\n",
    "            # Sample counts\n",
    "            **{f\"samples_{k}\": v for k, v in result.params.sample_counts.items()},\n",
    "            # Results\n",
    "            \"makespan\": result.makespan,\n",
    "            \"total_cost\": result.total_cost,\n",
    "            \"samples_per_year\": result.samples_per_year,\n",
    "            \"cost_per_sample\": result.cost_per_sample,\n",
    "            # Resource utilization\n",
    "            **{f\"utilization_{k}\": v for k, v in result.resource_utilization.items()},\n",
    "            # Error tracking\n",
    "            \"error\": result.error\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Log summary statistics\n",
    "    successful = df['error'].isna().sum()\n",
    "    total = len(df)\n",
    "    logging.info(f\"\\nResults Summary:\")\n",
    "    logging.info(f\"Total scenarios processed: {total}\")\n",
    "    logging.info(f\"Successful scenarios: {successful} ({successful/total*100:.1f}%)\")\n",
    "    logging.info(f\"Failed scenarios: {total-successful} ({(total-successful)/total*100:.1f}%)\")\n",
    "    if successful > 0:\n",
    "        logging.info(f\"Average samples per year: {df['samples_per_year'].mean():.1f}\")\n",
    "        logging.info(f\"Average cost per sample: ${df['cost_per_sample'].mean():.2f}\")\n",
    "        logging.info(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def run_capacity_analysis(\n",
    "    templates: List[dict],\n",
    "    resource_ranges: Dict[str, Tuple[int, int]],\n",
    "    sample_ranges: Dict[str, Tuple[int, int]],\n",
    "    resource_unit_costs: Dict[str, float],\n",
    "    step_sizes: Dict[str, int] = None,\n",
    "    output_file: str = \"capacity_results.csv\"\n",
    ") -> pd.DataFrame:\n",
    "          \n",
    "    \"\"\"Run complete capacity analysis across all scenarios.\"\"\"\n",
    "    # Validate ranges and step sizes\n",
    "    if not resource_ranges or not sample_ranges:\n",
    "        raise ValueError(\"Resource and sample ranges must not be empty.\")\n",
    "    \n",
    "    \n",
    "    # Generate scenarios\n",
    "    scenarios = generate_scenarios(resource_ranges, sample_ranges, step_sizes)\n",
    "    \n",
    "    # Ensure scenarios were generated\n",
    "    if not scenarios:\n",
    "        raise ValueError(\"No scenarios generated. Check ranges and step sizes.\")\n",
    "    \n",
    "    print(f\"Generated {len(scenarios)} scenarios\")\n",
    "    logging.info(f\"Generated {len(scenarios)} scenarios to evaluate\")\n",
    "    \n",
    "    # Convert scenarios to scheduler format\n",
    "    scheduler_scenarios = []\n",
    "    for scenario in scenarios:\n",
    "        scheduler_scenario = {\n",
    "            \"metals_count\": scenario.sample_counts.get(\"metals\", 0),\n",
    "            \"ceramics_count\": scenario.sample_counts.get(\"ceramics\", 0),\n",
    "            \"composites_count\": scenario.sample_counts.get(\"composites\", 0),\n",
    "            \"polymer_count\": scenario.sample_counts.get(\"polymers\", 0),\n",
    "            \"resource_capacities\": scenario.resource_levels\n",
    "        }\n",
    "        scheduler_scenarios.append(scheduler_scenario)\n",
    "    \n",
    "    # Run scenarios\n",
    "    results = run_scenarios_with_checkpoints(\n",
    "    scenarios=scenarios,\n",
    "    templates=templates,\n",
    "    resource_unit_costs=resource_unit_costs,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "    checkpoint_frequency=100\n",
    ")\n",
    "    \n",
    "    # Convert results to ScenarioResult format and save\n",
    "    final_results = process_and_save_results(scenarios, results, output_file)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def schedule_optimized(tasks, resource_capacities, max_solve_time=30):\n",
    "    \"\"\"\n",
    "    Optimized scheduler with:\n",
    "    - No work on weekends\n",
    "    - Maximum 10 hours per person per day\n",
    "    - Proper binary variable formulation\n",
    "    \"\"\"\n",
    "    # 1. Quick lookups and preprocessing\n",
    "    task_dict = {t[\"id\"]: t for t in tasks}\n",
    "    resource_tasks = defaultdict(list)\n",
    "    for t in tasks:\n",
    "        resource_tasks[t[\"resource\"]].append(t[\"id\"])\n",
    "    \n",
    "    # 2. Compute earliest starts using critical path\n",
    "    earliest_starts = compute_earliest_starts(tasks)\n",
    "    latest_completion = max(earliest_starts[t[\"id\"]] + t[\"duration\"] for t in tasks)\n",
    "    \n",
    "    # Add some buffer to the time horizon\n",
    "    time_horizon = latest_completion + 10\n",
    "    \n",
    "    # 3. Create model\n",
    "    model = pulp.LpProblem(\"Schedule\", pulp.LpMinimize)\n",
    "    \n",
    "    # 4. Variables\n",
    "    starts = {\n",
    "        t[\"id\"]: pulp.LpVariable(f'start_{t[\"id\"]}', \n",
    "                                earliest_starts[t[\"id\"]], \n",
    "                                time_horizon - t[\"duration\"],\n",
    "                                cat='Integer')\n",
    "        for t in tasks\n",
    "    }\n",
    "    \n",
    "    # Binary variables for task activity in each hour (more granular than before)\n",
    "    is_active = {\n",
    "        (task[\"id\"], h): pulp.LpVariable(\n",
    "            f'active_{task[\"id\"]}_{h}',\n",
    "            cat='Binary'\n",
    "        )\n",
    "        for task in tasks\n",
    "        for h in range(time_horizon * 24)  # Hours instead of periods\n",
    "    }\n",
    "    \n",
    "    makespan = pulp.LpVariable(\"makespan\", 0, time_horizon, cat='Integer')\n",
    "    \n",
    "    # 5. Objective: Minimize makespan\n",
    "    model += makespan\n",
    "    \n",
    "    # 6. Constraints\n",
    "    M = time_horizon * 24  # Big-M value adjusted for hours\n",
    "    \n",
    "    # Makespan constraint\n",
    "    for task in tasks:\n",
    "        model += starts[task[\"id\"]] * 24 + task[\"duration\"] * 24 <= makespan * 24\n",
    "\n",
    "    # Dependencies\n",
    "    for task in tasks:\n",
    "        for dep_id in task[\"dependencies\"]:\n",
    "            model += starts[task[\"id\"]] >= starts[dep_id] + task_dict[dep_id][\"duration\"]\n",
    "    \n",
    "    # Activity constraints with hour granularity\n",
    "    for task in tasks:\n",
    "        task_id = task[\"id\"]\n",
    "        duration = task[\"duration\"] * 24  # Convert to hours\n",
    "        \n",
    "        for h in range(time_horizon * 24):\n",
    "            # Task active in this hour if it starts before hour end and ends after hour start\n",
    "            model += starts[task_id] * 24 <= h + M * (1 - is_active[task_id, h])\n",
    "            model += starts[task_id] * 24 + duration >= h - M * (1 - is_active[task_id, h])\n",
    "    \n",
    "    # Weekend constraints - no work on Saturday (day % 7 == 5) or Sunday (day % 7 == 6)\n",
    "    for task in tasks:\n",
    "        task_id = task[\"id\"]\n",
    "        for h in range(time_horizon * 24):\n",
    "            day = h // 24\n",
    "            if day % 7 in [5, 6]:  # Weekend\n",
    "                model += is_active[task_id, h] == 0\n",
    "    \n",
    "    # Daily work hour constraints - maximum 10 hours per person per day\n",
    "    for res, res_tasks_list in resource_tasks.items():\n",
    "        if not res_tasks_list or res not in resource_capacities:\n",
    "            continue\n",
    "            \n",
    "        for day in range(time_horizon):\n",
    "            day_start = day * 24\n",
    "            day_end = (day + 1) * 24\n",
    "            \n",
    "            # Sum of active hours in day must not exceed 10 per resource unit\n",
    "            model += (pulp.lpSum(is_active[task_id, h] \n",
    "                               for task_id in res_tasks_list\n",
    "                               for h in range(day_start, day_end)\n",
    "                               ) <= 10 * resource_capacities[res])\n",
    "    \n",
    "    # 7. Solve\n",
    "    solver = pulp.PULP_CBC_CMD(msg=1, timeLimit=max_solve_time)\n",
    "    status = model.solve(solver)\n",
    "    \n",
    "    if status != pulp.LpStatusOptimal:\n",
    "        return None, {}\n",
    "    \n",
    "    # 8. Extract results - converting back to days for compatibility\n",
    "    start_times = {t[\"id\"]: int(starts[t[\"id\"]].value()) for t in tasks}\n",
    "    makespan_val = int(makespan.value())\n",
    "    \n",
    "    return makespan_val, start_times\n",
    "\n",
    "def process_scenario(scenario, templates, resource_unit_costs, scenario_num=0, total_scenarios=0):\n",
    "    \"\"\"Process a single scenario with detailed logging.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"\\nProcessing scenario {scenario_num}/{total_scenarios}\")\n",
    "        logging.info(\"Scenario configuration:\")\n",
    "        logging.info(f\"Sample counts: Metals={scenario.get('metals_count', 0)}, \"\n",
    "                    f\"Ceramics={scenario.get('ceramics_count', 0)}, \"\n",
    "                    f\"Composites={scenario.get('composites_count', 0)}, \"\n",
    "                    f\"Polymers={scenario.get('polymer_count', 0)}\")\n",
    "        logging.info(f\"Resource capacities: {scenario['resource_capacities']}\")\n",
    "        \n",
    "        # Generate tasks for this scenario\n",
    "        all_tasks = []\n",
    "        scenario_values = [\n",
    "            scenario.get('metals_count', 0),\n",
    "            scenario.get('ceramics_count', 0),\n",
    "            scenario.get('composites_count', 0),\n",
    "            scenario.get('polymer_count', 0)\n",
    "        ]\n",
    "        \n",
    "        for template, count, prefix in zip(templates, scenario_values, \n",
    "                                         [\"MET_\", \"CER_\", \"COMP_\", \"POLY_\"]):\n",
    "            if count > 0:\n",
    "                new_tasks = replicate_wbs_optimized(template, count, prefix)\n",
    "                all_tasks.extend(new_tasks)\n",
    "                logging.info(f\"Generated {len(new_tasks)} tasks for {prefix.strip('_')} template\")\n",
    "        \n",
    "        # Log task generation summary\n",
    "        logging.info(f\"Total tasks generated: {len(all_tasks)}\")\n",
    "        \n",
    "        # Resource usage summary\n",
    "        resource_task_counts = {}\n",
    "        for task in all_tasks:\n",
    "            resource_task_counts[task[\"resource\"]] = resource_task_counts.get(task[\"resource\"], 0) + 1\n",
    "        logging.info(\"Tasks per resource:\")\n",
    "        for resource, count in resource_task_counts.items():\n",
    "            logging.info(f\"  {resource}: {count} tasks\")\n",
    "        \n",
    "        # Validate dependencies\n",
    "        task_ids = {task[\"id\"] for task in all_tasks}\n",
    "        for task in all_tasks:\n",
    "            for dep in task[\"dependencies\"]:\n",
    "                if dep not in task_ids:\n",
    "                    raise ValueError(f\"Task {task['id']} references missing dependency {dep}\")\n",
    "        \n",
    "        # If no tasks, return early\n",
    "        if not all_tasks:\n",
    "            logging.warning(\"No tasks generated for this scenario\")\n",
    "            return {\n",
    "                'scenario': scenario,\n",
    "                'makespan': 0,\n",
    "                'total_cost': 0,\n",
    "                'usage': {},\n",
    "                'error': None\n",
    "            }\n",
    "        \n",
    "        # Schedule tasks\n",
    "        logging.info(\"Starting scheduling optimization...\")\n",
    "        makespan, starts = schedule_optimized(\n",
    "            tasks=all_tasks,\n",
    "            resource_capacities=scenario['resource_capacities']\n",
    "        )\n",
    "        \n",
    "        if makespan is None:\n",
    "            logging.error(\"No feasible schedule found\")\n",
    "            return {\n",
    "                'scenario': scenario,\n",
    "                'makespan': None,\n",
    "                'total_cost': 0,\n",
    "                'usage': {},\n",
    "                'error': \"No feasible schedule found\"\n",
    "            }\n",
    "        \n",
    "        logging.info(f\"Schedule found with makespan: {makespan}\")\n",
    "        \n",
    "        # Calculate resource usage and costs\n",
    "        usage = defaultdict(lambda: defaultdict(int))\n",
    "        for task_id, start_time in starts.items():\n",
    "            task = next(t for t in all_tasks if t[\"id\"] == task_id)\n",
    "            for t in range(start_time, start_time + task[\"duration\"]):\n",
    "                usage[task[\"resource\"]][t] += 1\n",
    "        \n",
    "        total_cost = sum(\n",
    "            count * resource_unit_costs.get(resource, 0)\n",
    "            for resource, times in usage.items()\n",
    "            for count in times.values()\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Total cost calculated: {total_cost}\")\n",
    "        \n",
    "        # Calculate resource utilization\n",
    "        for resource, times in usage.items():\n",
    "            max_possible_usage = scenario['resource_capacities'][resource] * makespan\n",
    "            total_usage = sum(times.values())\n",
    "            utilization = total_usage / max_possible_usage if max_possible_usage > 0 else 0\n",
    "            logging.info(f\"Resource utilization - {resource}: {utilization:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'scenario': scenario,\n",
    "            'makespan': makespan,\n",
    "            'total_cost': total_cost,\n",
    "            'usage': dict(usage),\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing scenario: {str(e)}\")\n",
    "        import traceback\n",
    "        logging.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return {\n",
    "            'scenario': scenario,\n",
    "            'makespan': None,\n",
    "            'total_cost': 0,\n",
    "            'usage': {},\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def run_scenarios_optimized(scenarios, templates, resource_unit_costs, max_workers=4):\n",
    "    \"\"\"Run all scenarios in parallel with progress tracking.\"\"\"\n",
    "    results = []\n",
    "    total = len(scenarios)\n",
    "    completed = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all scenarios\n",
    "        future_to_scenario = {\n",
    "            executor.submit(\n",
    "                process_scenario, \n",
    "                s, \n",
    "                templates, \n",
    "                resource_unit_costs,\n",
    "                i + 1,  # scenario number\n",
    "                total   # total scenarios\n",
    "            ): i \n",
    "            for i, s in enumerate(scenarios)\n",
    "        }\n",
    "        \n",
    "        # Process results with progress bar\n",
    "        with tqdm(total=total, desc=\"Processing scenarios\") as pbar:\n",
    "            for future in as_completed(future_to_scenario):\n",
    "                scenario_idx = future_to_scenario[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    completed += 1\n",
    "                    \n",
    "                    # Log progress every 10 scenarios or when there's an error\n",
    "                    if completed % 10 == 0 or result.get('error'):\n",
    "                        elapsed = time.time() - start_time\n",
    "                        rate = completed / elapsed\n",
    "                        eta = (total - completed) / rate if rate > 0 else 0\n",
    "                        logging.info(\n",
    "                            f\"\\nProgress Update:\\n\"\n",
    "                            f\"Completed {completed}/{total} scenarios \"\n",
    "                            f\"({completed/total*100:.1f}%)\\n\"\n",
    "                            f\"Rate: {rate:.1f} scenarios/sec\\n\"\n",
    "                            f\"ETA: {eta/60:.1f} minutes\"\n",
    "                        )\n",
    "                        \n",
    "                        if result.get('error'):\n",
    "                            logging.error(\n",
    "                                f\"Scenario {scenario_idx + 1} failed: {result.get('error')}\\n\"\n",
    "                                f\"Configuration: {scenarios[scenario_idx]}\"\n",
    "                            )\n",
    "                        elif result.get('makespan'):\n",
    "                            logging.info(\n",
    "                                f\"Scenario {scenario_idx + 1} succeeded:\\n\"\n",
    "                                f\"Makespan: {result['makespan']}\\n\"\n",
    "                                f\"Total Cost: {result['total_cost']:.2f}\"\n",
    "                            )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing scenario {scenario_idx + 1}: {str(e)}\")\n",
    "                    results.append({\n",
    "                        'scenario': scenarios[scenario_idx],\n",
    "                        'makespan': None,\n",
    "                        'total_cost': 0,\n",
    "                        'usage': {},\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Final summary\n",
    "    successful = sum(1 for r in results if not r.get('error'))\n",
    "    logging.info(\n",
    "        f\"\\nFinal Results Summary:\\n\"\n",
    "        f\"Total scenarios: {total}\\n\"\n",
    "        f\"Successful: {successful} ({successful/total*100:.1f}%)\\n\"\n",
    "        f\"Failed: {total-successful} ({(total-successful)/total*100:.1f}%)\\n\"\n",
    "        f\"Total time: {time.time() - start_time:.1f} seconds\"\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Helper function for task replication\n",
    "def replicate_wbs_optimized(template, count, prefix):\n",
    "    \"\"\"Optimized WBS template replication.\"\"\"\n",
    "    if count == 0:\n",
    "        return []\n",
    "    replicated = []\n",
    "    for n in range(count):\n",
    "        instance_suffix = f\"_{n}\"\n",
    "        for task in template:\n",
    "            replicated.append({\n",
    "                \"id\": f\"{prefix}{task['id']}{instance_suffix}\",\n",
    "                \"duration\": task[\"duration\"],\n",
    "                \"dependencies\": [f\"{prefix}{dep}{instance_suffix}\" \n",
    "                               for dep in task[\"dependencies\"]],\n",
    "                \"resource\": task[\"resource\"]\n",
    "            })\n",
    "    return replicated\n",
    "\n",
    "def save_results(results: List[ScenarioResult], filename: str):\n",
    "    \"\"\"Save scenario results to CSV\"\"\"\n",
    "    records = []\n",
    "    for result in results:\n",
    "        record = {\n",
    "            # Resource levels\n",
    "            **{f\"resource_{k}\": v for k, v in result.params.resource_levels.items()},\n",
    "            # Sample counts\n",
    "            **{f\"samples_{k}\": v for k, v in result.params.sample_counts.items()},\n",
    "            # Results\n",
    "            \"makespan\": result.makespan,\n",
    "            \"total_cost\": result.total_cost,\n",
    "            \"samples_per_year\": result.samples_per_year,\n",
    "            \"cost_per_sample\": result.cost_per_sample,\n",
    "            # Resource utilization\n",
    "            **{f\"utilization_{k}\": v for k, v in result.resource_utilization.items()}\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(filename, index=False)\n",
    "    return df\n",
    "\n",
    "    \n",
    "import pulp\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# [Previous scenario generation code remains the same through the classes]\n",
    "@dataclass\n",
    "class ScenarioParams:\n",
    "    resource_levels: Dict[str, int]\n",
    "    sample_counts: Dict[str, int]\n",
    "\n",
    "@dataclass\n",
    "class ScenarioResult:\n",
    "    params: ScenarioParams\n",
    "    makespan: int\n",
    "    total_cost: float\n",
    "    samples_per_year: int\n",
    "    cost_per_sample: float\n",
    "    resource_utilization: Dict[str, float]\n",
    "\n",
    "def run_capacity_analysis(\n",
    "    templates: List[dict],\n",
    "    resource_ranges: Dict[str, Tuple[int, int]],\n",
    "    sample_ranges: Dict[str, Tuple[int, int]],\n",
    "    resource_unit_costs: Dict[str, float],\n",
    "    step_sizes: Dict[str, int] = None,  # Optional - set to None to use step size of 1\n",
    "    output_file: str = \"capacity_results.csv\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run complete capacity analysis across all scenarios.\n",
    "    \n",
    "    Args:\n",
    "        templates: List of WBS templates [metals, ceramics, composites, polymers]\n",
    "        resource_ranges: Dict of resource min/max e.g., {\"Admin\": (1,3)}\n",
    "        sample_ranges: Dict of sample type min/max e.g., {\"metals\": (0,10)}\n",
    "        resource_unit_costs: Dict of costs per resource unit\n",
    "        step_sizes: Dict of step sizes for each parameter (optional)\n",
    "        output_file: Where to save results\n",
    "    \"\"\"\n",
    "    # Generate scenarios\n",
    "    scenarios = generate_scenarios(resource_ranges, sample_ranges, step_sizes)\n",
    "    print(f\"Generated {len(scenarios)} scenarios to evaluate\")\n",
    "    \n",
    "    # Convert scenarios to format needed by scheduler\n",
    "    scheduler_scenarios = []\n",
    "    for scenario in scenarios:\n",
    "        scheduler_scenario = {\n",
    "            \"metals_count\": scenario.sample_counts.get(\"metals\", 0),\n",
    "            \"ceramics_count\": scenario.sample_counts.get(\"ceramics\", 0),\n",
    "            \"composites_count\": scenario.sample_counts.get(\"composites\", 0),\n",
    "            \"polymer_count\": scenario.sample_counts.get(\"polymers\", 0),\n",
    "            \"resource_capacities\": scenario.resource_levels\n",
    "        }\n",
    "        scheduler_scenarios.append(scheduler_scenario)\n",
    "    \n",
    "    # Run scenarios\n",
    "    start_time = time.time()\n",
    "    results = run_scenarios_optimized(scheduler_scenarios, templates, resource_unit_costs)\n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {len(scenarios)} scenarios in {end_time - start_time:.1f} seconds\")\n",
    "    \n",
    "    # Convert results to ScenarioResult format\n",
    "    final_results = []\n",
    "    for scenario, result in zip(scenarios, results):\n",
    "        if result['makespan'] is None:  # Handle infeasible scenarios\n",
    "            continue\n",
    "            \n",
    "        # Calculate samples per year (assuming makespan is in days)\n",
    "        total_samples = sum(scenario.sample_counts.values())\n",
    "        samples_per_year = int(365 * total_samples / result['makespan'])\n",
    "        \n",
    "        # Calculate cost per sample\n",
    "        cost_per_sample = result['total_cost'] / total_samples if total_samples > 0 else 0\n",
    "        \n",
    "        # Calculate resource utilization\n",
    "        utilization = {}\n",
    "        for resource, usage_dict in result['usage'].items():\n",
    "            max_possible_usage = scenario.resource_levels[resource] * result['makespan']\n",
    "            total_usage = sum(usage_dict.values())\n",
    "            utilization[resource] = total_usage / max_possible_usage if max_possible_usage > 0 else 0\n",
    "        \n",
    "        final_results.append(ScenarioResult(\n",
    "            params=scenario,\n",
    "            makespan=result['makespan'],\n",
    "            total_cost=result['total_cost'],\n",
    "            samples_per_year=samples_per_year,\n",
    "            cost_per_sample=cost_per_sample,\n",
    "            resource_utilization=utilization\n",
    "        ))\n",
    "    \n",
    "    # Save results\n",
    "    df = save_results(final_results, output_file)\n",
    "    return df\n",
    "\n",
    "#\n",
    "# ============= YOUR WBS TEMPLATES =============\n",
    "#\n",
    "metals_template = [\n",
    "    # ------ Sample Management ------\n",
    "    {\"id\": \"1.1\", \"name\": \"Receive and Log Samples\",        \"duration\": 4, \"dependencies\": [],        \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.2\", \"name\": \"Assign Unique Identifiers\",      \"duration\": 2, \"dependencies\": [\"1.1\"],   \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.3\", \"name\": \"Photograph Initial Condition\",   \"duration\": 2, \"dependencies\": [\"1.2\"],   \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"1.4\", \"name\": \"Write Analysis Plan\",            \"duration\": 3, \"dependencies\": [\"1.3\"],   \"resource\": \"Materials Scientist (Metals)\"},\n",
    "    {\"id\": \"1.5\", \"name\": \"Store Samples Appropriately\",    \"duration\": 1, \"dependencies\": [\"1.4\"],   \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.6\", \"name\": \"Document Chain of Custody\",      \"duration\": 1, \"dependencies\": [\"1.5\"],   \"resource\": \"Administrative Support Specialist\"},\n",
    "    # ------ Sample Preparation ------\n",
    "    {\"id\": \"2.1\", \"name\": \"Cut Subsamples for Mounting\",    \"duration\": 3, \"dependencies\": [\"1.6\"],   \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.2\", \"name\": \"Cut Subsamples for Light El.\",   \"duration\": 3, \"dependencies\": [\"1.6\"],   \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.3\", \"name\": \"Cut for ICP-OES/ICP-MS\",         \"duration\": 3, \"dependencies\": [\"1.6\"],   \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.4\", \"name\": \"Clean Cut Subsamples\",           \"duration\": 2, \"dependencies\": [\"2.1\",\"2.2\",\"2.3\"], \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.5\", \"name\": \"Mount Subsample in Epoxy\",       \"duration\": 3, \"dependencies\": [\"2.1\"],   \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.6\", \"name\": \"Grind & Polish Mounted Subsmpl\", \"duration\": 4, \"dependencies\": [\"2.5\"],   \"resource\": \"Materials Scientist (Metals)\"},\n",
    "    {\"id\": \"2.7\", \"name\": \"Digest Sample with Acid/MW\",     \"duration\": 4, \"dependencies\": [\"2.4\"],   \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    {\"id\": \"2.8\", \"name\": \"Prep ICP Standards/Blanks\",      \"duration\": 2, \"dependencies\": [],        \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    {\"id\": \"2.9\", \"name\": \"Coat Sample for SEM/EBSD\",       \"duration\": 2, \"dependencies\": [\"2.6\"],   \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.10\",\"name\": \"Label & Catalog Prepared Smpls\", \"duration\": 2, \"dependencies\": [\"2.3\",\"2.6\",\"2.7\",\"2.9\"], \"resource\": \"Administrative Support Specialist\"},\n",
    "    # ------ Sample Analysis ------\n",
    "    {\"id\": \"3.1\", \"name\": \"Optical Microscope\",             \"duration\": 2, \"dependencies\": [\"2.6\"],   \"resource\": \"Materials Scientist (Metals)\"},\n",
    "    {\"id\": \"3.2\", \"name\": \"SEM Imaging\",                    \"duration\": 4, \"dependencies\": [\"2.9\"],   \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"3.3\", \"name\": \"SEM-EDS Bulk Composition\",       \"duration\": 4, \"dependencies\": [\"3.2\"],   \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"3.4\", \"name\": \"SEM-EBSD Grain Analysis\",        \"duration\": 4, \"dependencies\": [\"3.2\"],   \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"3.5\", \"name\": \"Density via Pycnometer\",         \"duration\": 3, \"dependencies\": [\"2.4\"],   \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"3.6\", \"name\": \"Composition XRF\",                \"duration\": 3, \"dependencies\": [\"2.4\"],   \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"3.7\", \"name\": \"Composition SparkOES\",           \"duration\": 3, \"dependencies\": [\"2.6\"],   \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"3.8\", \"name\": \"Light Elements (CHNOS)\",         \"duration\": 3, \"dependencies\": [\"2.4\"],   \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    {\"id\": \"3.9\", \"name\": \"Hardness Microindentation\",      \"duration\": 3, \"dependencies\": [\"2.6\"],   \"resource\": \"Materials Scientist (Metals)\"},\n",
    "    {\"id\": \"3.10\",\"name\": \"ICP-MS/ICP-OES\",                 \"duration\": 8, \"dependencies\": [],        \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    # ------ Data Analysis ------\n",
    "    {\"id\": \"4.1\", \"name\": \"Compile Analytical Results\",     \"duration\": 2, \"dependencies\": [\"3.1\",\"3.2\",\"3.3\",\"3.4\",\"3.5\",\"3.6\",\"3.7\",\"3.8\",\"3.9\"],     \"resource\": \"Materials Scientist (Metals)\"},\n",
    "    {\"id\": \"4.2\", \"name\": \"Cross-Validate Composition\",     \"duration\": 6, \"dependencies\": [\"3.3\",\"3.4\",\"3.5\",\"3.6\",\"3.7\",\"3.8\",\"3.9\"], \"resource\": \"Materials Scientist (Metals)\"},\n",
    "    {\"id\": \"4.3\", \"name\": \"DB Match or Similar Alloy\",      \"duration\": 4, \"dependencies\": [],        \"resource\": \"Materials Scientist (Metals)\"},\n",
    "    {\"id\": \"4.4\", \"name\": \"Prepare Final Figures\",          \"duration\": 3, \"dependencies\": [\"4.3\"],   \"resource\": \"Materials Scientist (Metals)\"},\n",
    "    {\"id\": \"4.5\", \"name\": \"Perform Peer Review\",            \"duration\": 2, \"dependencies\": [\"4.4\"],   \"resource\": \"Materials Scientist (Metals)\"},\n",
    "    {\"id\": \"4.6\", \"name\": \"QA Review of Data/Results\",      \"duration\": 1, \"dependencies\": [\"4.5\"],   \"resource\": \"Quality Assurance Officer\"},\n",
    "    {\"id\": \"4.7\", \"name\": \"Draft Report to Client\",         \"duration\": 2, \"dependencies\": [\"4.6\"],   \"resource\": \"Project Manager\"},\n",
    "    {\"id\": \"4.8\", \"name\": \"Update Report Per Feedback\",     \"duration\": 2, \"dependencies\": [\"4.7\"],   \"resource\": \"Project Manager\"},\n",
    "    {\"id\": \"4.9\", \"name\": \"Enter Results into Database\",    \"duration\": 2, \"dependencies\": [\"4.8\"],   \"resource\": \"Administrative Support Specialist\"},\n",
    "]\n",
    "\n",
    "ceramics_template = [\n",
    "    # ------ Sample Management ------\n",
    "    {\"id\": \"1.1\", \"name\": \"Receive and Log Samples\",         \"duration\": 4, \"dependencies\": [],       \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.2\", \"name\": \"Assign Unique Identifiers\",       \"duration\": 2, \"dependencies\": [\"1.1\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.3\", \"name\": \"Photograph Initial Condition\",    \"duration\": 2, \"dependencies\": [\"1.2\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"1.4\", \"name\": \"Write Analysis Plan\",             \"duration\": 3, \"dependencies\": [\"1.3\"],  \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"1.5\", \"name\": \"Store Samples Appropriately\",     \"duration\": 1, \"dependencies\": [\"1.4\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.6\", \"name\": \"Document Chain of Custody\",       \"duration\": 1, \"dependencies\": [\"1.5\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    # ------ Sample Preparation ------\n",
    "    {\"id\": \"2.1\", \"name\": \"Cut Subsamples for Mounting\",     \"duration\": 3, \"dependencies\": [\"1.6\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.2\", \"name\": \"Cut Subsamples for ICP\",          \"duration\": 3, \"dependencies\": [\"1.6\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.3\", \"name\": \"Clean Cut Subsamples\",            \"duration\": 2, \"dependencies\": [\"2.1\",\"2.2\"], \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.4\", \"name\": \"Mount Subsample in Epoxy\",        \"duration\": 3, \"dependencies\": [\"2.3\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.5\", \"name\": \"Crush & Sieve for Powder\",        \"duration\": 4, \"dependencies\": [\"2.3\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.6\", \"name\": \"Grind & Polish Mounted Subsmpl\",  \"duration\": 4, \"dependencies\": [\"2.4\"],  \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"2.7\", \"name\": \"Digest Sample (Acid/MW)\",         \"duration\": 4, \"dependencies\": [\"2.3\"],  \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    {\"id\": \"2.8\", \"name\": \"Prep ICP Standards/Blanks\",       \"duration\": 2, \"dependencies\": [],       \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    {\"id\": \"2.9\", \"name\": \"Coat Sample for SEM/EBSD\",        \"duration\": 2, \"dependencies\": [\"2.6\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.10\",\"name\": \"Label & Catalog Prepared Smpls\",  \"duration\": 2, \"dependencies\": [\"2.5\",\"2.3\",\"2.6\",\"2.7\",\"2.9\"], \"resource\": \"Administrative Support Specialist\"},\n",
    "    # ------ Sample Analysis ------\n",
    "    {\"id\": \"3.1\", \"name\": \"FTIR for Composition\",            \"duration\": 4, \"dependencies\": [\"2.10\"], \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"3.2\", \"name\": \"Raman for Crystalline Phases\",    \"duration\": 4, \"dependencies\": [\"2.10\"], \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"3.3\", \"name\": \"SEM Imaging (Microstructure)\",     \"duration\": 6, \"dependencies\": [\"2.10\"], \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"3.4\", \"name\": \"XRD for Phase Info\",               \"duration\": 5, \"dependencies\": [\"2.10\"], \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"3.5\", \"name\": \"TGA for Thermal Stability\",        \"duration\": 5, \"dependencies\": [\"2.10\"], \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"3.6\", \"name\": \"DSC for Thermal Transitions\",      \"duration\": 5, \"dependencies\": [\"2.10\"], \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"3.7\", \"name\": \"Microindentation (Knoop)\",         \"duration\": 6, \"dependencies\": [\"2.10\"], \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"3.8\", \"name\": \"SEM-EDS for Elemental Comp\",       \"duration\": 6, \"dependencies\": [\"2.10\"], \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"3.9\", \"name\": \"ICP-OES/ICP-MS for Trace Elem\",    \"duration\": 6, \"dependencies\": [\"2.10\"], \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    # ------ Data Analysis ------\n",
    "    {\"id\": \"4.1\", \"name\": \"Compile Analytical Results\",       \"duration\": 2, \"dependencies\": [\"3.3\",\"3.4\",\"3.5\",\"3.6\",\"3.7\",\"3.8\",\"3.9\"],    \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"4.2\", \"name\": \"Interpret Data (Ceramic Props)\",   \"duration\": 6, \"dependencies\": [\"4.1\"],  \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"4.3\", \"name\": \"Cross-Validate Across Techniques\", \"duration\": 6, \"dependencies\": [\"4.2\"],  \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"4.4\", \"name\": \"Prepare Final Figures\",            \"duration\": 3, \"dependencies\": [\"4.3\"],  \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"4.5\", \"name\": \"Perform Peer Review\",             \"duration\": 2, \"dependencies\": [\"4.4\"],  \"resource\": \"Materials Scientist (Ceramics)\"},\n",
    "    {\"id\": \"4.6\", \"name\": \"QA Review of Data/Results\",       \"duration\": 2, \"dependencies\": [\"4.5\"],  \"resource\": \"Quality Assurance Officer\"},\n",
    "    {\"id\": \"4.7\", \"name\": \"Draft Report to Client\",          \"duration\": 2, \"dependencies\": [\"4.6\"],  \"resource\": \"Project Manager\"},\n",
    "    {\"id\": \"4.8\", \"name\": \"Update Report Per Feedback\",      \"duration\": 2, \"dependencies\": [\"4.7\"],  \"resource\": \"Project Manager\"},\n",
    "    {\"id\": \"4.9\", \"name\": \"Enter Results into Database\",     \"duration\": 2, \"dependencies\": [\"4.8\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "]\n",
    "\n",
    "composites_template = [\n",
    "    # ------ Sample Management ------\n",
    "    {\"id\": \"1.1\", \"name\": \"Receive and Log Samples\",         \"duration\": 4, \"dependencies\": [],       \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.2\", \"name\": \"Assign Unique Identifiers\",       \"duration\": 2, \"dependencies\": [\"1.1\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.3\", \"name\": \"Photograph Initial Condition\",    \"duration\": 2, \"dependencies\": [\"1.2\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"1.4\", \"name\": \"Write Analysis Plan\",             \"duration\": 3, \"dependencies\": [\"1.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"1.5\", \"name\": \"Store Samples Appropriately\",     \"duration\": 1, \"dependencies\": [\"1.4\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.6\", \"name\": \"Document Chain of Custody\",       \"duration\": 1, \"dependencies\": [\"1.5\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    # ------ Sample Preparation ------\n",
    "    {\"id\": \"2.1\", \"name\": \"Section and Polish\",             \"duration\": 3, \"dependencies\": [\"1.6\"],  \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"2.2\", \"name\": \"Embed Samples in Resin\",         \"duration\": 3, \"dependencies\": [\"1.6\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.3\", \"name\": \"Label & Catalog Prepared Smpls\", \"duration\": 3, \"dependencies\": [\"1.6\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    # ------ Sample Analysis ------\n",
    "    {\"id\": \"3.1\", \"name\": \"FTIR for Matrix\",                \"duration\": 4, \"dependencies\": [\"2.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.2\", \"name\": \"Raman for Fibers\",               \"duration\": 4, \"dependencies\": [\"2.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.3\", \"name\": \"Raman for Matrix\",               \"duration\": 4, \"dependencies\": [\"2.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.4.1\",\"name\": \"SEM-EDS Fiber Comp\",            \"duration\": 6, \"dependencies\": [\"2.3\"],  \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"3.4.2\",\"name\": \"SEM-EDS Matrix Comp\",           \"duration\": 6, \"dependencies\": [\"2.3\"],  \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"3.5\", \"name\": \"TGA (Thermal Stability/Filler)\", \"duration\": 5, \"dependencies\": [\"2.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.6\", \"name\": \"DSC (Polymer Transition)\",       \"duration\": 5, \"dependencies\": [\"2.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.7\", \"name\": \"PYGCMS (Matrix Composition)\",    \"duration\": 6, \"dependencies\": [\"2.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.8\", \"name\": \"ICP-OES/ICP-MS (Elements)\",      \"duration\": 8, \"dependencies\": [\"2.3\"],  \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    {\"id\": \"3.9\", \"name\": \"Fiber Analysis (Optical/SEM)\",   \"duration\": 8, \"dependencies\": [\"2.3\"],  \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"3.10\",\"name\": \"Measure Density (Pycnometer)\",   \"duration\": 3, \"dependencies\": [\"2.3\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    # ------ Data Analysis ------\n",
    "    {\"id\": \"4.1\", \"name\": \"Cross-Validate Composition\",     \"duration\": 6, \"dependencies\": [\"3.3\",\"3.4.1\",\"3.4.2\",\"3.5\",\"3.6\",\"3.7\",\"3.8\",\"3.9\"],    \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"4.2\", \"name\": \"Prepare Final Figures\",          \"duration\": 3, \"dependencies\": [\"4.1\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"4.3\", \"name\": \"Perform Peer Review\",            \"duration\": 2, \"dependencies\": [\"4.2\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"4.4\", \"name\": \"QA Review of Data/Results\",      \"duration\": 1, \"dependencies\": [\"4.3\"],  \"resource\": \"Quality Assurance Officer\"},\n",
    "    {\"id\": \"4.5\", \"name\": \"Draft Report to Client\",         \"duration\": 2, \"dependencies\": [\"4.4\"],  \"resource\": \"Project Manager\"},\n",
    "    {\"id\": \"4.6\", \"name\": \"Update Report Per Feedback\",     \"duration\": 2, \"dependencies\": [\"4.5\"],  \"resource\": \"Project Manager\"},\n",
    "    {\"id\": \"4.7\", \"name\": \"Enter Results into Database\",    \"duration\": 2, \"dependencies\": [\"4.6\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "]\n",
    "\n",
    "polymers_template = [\n",
    "    # ------ Sample Management ------\n",
    "    {\"id\": \"1.1\", \"name\": \"Receive and Log Samples\",        \"duration\": 4, \"dependencies\": [],       \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.2\", \"name\": \"Assign Unique Identifiers\",      \"duration\": 2, \"dependencies\": [\"1.1\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.3\", \"name\": \"Photograph Initial Condition\",   \"duration\": 2, \"dependencies\": [\"1.2\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"1.4\", \"name\": \"Write Analysis Plan\",            \"duration\": 3, \"dependencies\": [\"1.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"1.5\", \"name\": \"Store Samples Appropriately\",    \"duration\": 1, \"dependencies\": [\"1.4\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    {\"id\": \"1.6\", \"name\": \"Document Chain of Custody\",      \"duration\": 1, \"dependencies\": [\"1.5\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "    # ------ Sample Preparation ------\n",
    "    {\"id\": \"2.1\", \"name\": \"Cut Subsamples for Mounting\",    \"duration\": 3, \"dependencies\": [],       \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.2\", \"name\": \"Cut Subsamples for ICP\",         \"duration\": 3, \"dependencies\": [],       \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.3\", \"name\": \"Clean Cut Subsamples\",           \"duration\": 2, \"dependencies\": [\"2.1\",\"2.2\"], \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.4\", \"name\": \"Mount Subsample in Epoxy\",       \"duration\": 3, \"dependencies\": [\"2.1\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.5\", \"name\": \"Grind & Polish Mounted Subsmpl\", \"duration\": 4, \"dependencies\": [\"2.4\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"2.6\", \"name\": \"Digest Sample (Acid/MW)\",        \"duration\": 4, \"dependencies\": [\"2.3\"],  \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    {\"id\": \"2.7\", \"name\": \"Prep ICP Standards/Blanks\",      \"duration\": 2, \"dependencies\": [],       \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    {\"id\": \"2.8\", \"name\": \"Coat Sample for SEM/EBSD\",       \"duration\": 2, \"dependencies\": [\"2.5\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"2.9\", \"name\": \"Label & Catalog Prepared Smpls\", \"duration\": 2, \"dependencies\": [\"2.2\",\"2.5\",\"2.6\",\"2.8\"], \"resource\": \"Administrative Support Specialist\"},\n",
    "    # ------ Sample Analysis ------\n",
    "    {\"id\": \"3.1\", \"name\": \"Optical Microscope\",             \"duration\": 2, \"dependencies\": [\"2.5\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.2\", \"name\": \"SEM Imaging\",                    \"duration\": 4, \"dependencies\": [\"2.8\"],  \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"3.3\", \"name\": \"SEM-EDS (Polymer Comp)\",         \"duration\": 4, \"dependencies\": [\"3.2\"],  \"resource\": \"Advanced Imaging Specialist\"},\n",
    "    {\"id\": \"3.4\", \"name\": \"FTIR for Composition\",           \"duration\": 4, \"dependencies\": [\"2.5\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.5\", \"name\": \"Raman for Composition\",          \"duration\": 4, \"dependencies\": [\"2.5\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.6\", \"name\": \"Measure Density (Pycnometer)\",   \"duration\": 3, \"dependencies\": [\"2.3\"],  \"resource\": \"Laboratory Technician\"},\n",
    "    {\"id\": \"3.7\", \"name\": \"Hardness Microindentation\",      \"duration\": 3, \"dependencies\": [\"2.5\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.8\", \"name\": \"TGA (Thermal Stability/Filler)\", \"duration\": 5, \"dependencies\": [\"2.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.9\", \"name\": \"DSC (Polymer Transition)\",       \"duration\": 5, \"dependencies\": [\"2.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.10\",\"name\": \"PYGCMS (Matrix)\",                \"duration\": 6, \"dependencies\": [\"2.3\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"3.11\",\"name\": \"ICP-MS/ICP-OES\",                 \"duration\": 8, \"dependencies\": [\"2.6\",\"2.7\"], \"resource\": \"Analytical Instrument Specialist\"},\n",
    "    # ------ Data Analysis ------\n",
    "    {\"id\": \"4.1\", \"name\": \"Cross-Validate Composition\",     \"duration\": 6, \"dependencies\": [\"3.3\",\"3.4\",\"3.5\",\"3.6\",\"3.7\",\"3.8\",\"3.9\"],    \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"4.2\", \"name\": \"Prepare Final Figures\",          \"duration\": 3, \"dependencies\": [\"4.1\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"4.3\", \"name\": \"Perform Peer Review\",            \"duration\": 2, \"dependencies\": [\"4.2\"],  \"resource\": \"Materials Scientist (Composites)\"},\n",
    "    {\"id\": \"4.4\", \"name\": \"QA Review of Data/Results\",      \"duration\": 1, \"dependencies\": [\"4.3\"],  \"resource\": \"Quality Assurance Officer\"},\n",
    "    {\"id\": \"4.5\", \"name\": \"Draft Report to Client\",         \"duration\": 2, \"dependencies\": [\"4.4\"],  \"resource\": \"Project Manager\"},\n",
    "    {\"id\": \"4.6\", \"name\": \"Update Report Per Feedback\",     \"duration\": 2, \"dependencies\": [\"4.5\"],  \"resource\": \"Project Manager\"},\n",
    "    {\"id\": \"4.7\", \"name\": \"Enter Results into Database\",    \"duration\": 2, \"dependencies\": [\"4.6\"],  \"resource\": \"Administrative Support Specialist\"},\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Starting capacity analysis\")\n",
    "    \n",
    "    templates = [metals_template, ceramics_template, composites_template, polymers_template]\n",
    "    \n",
    "    resource_ranges = {\n",
    "        \"Administrative Support Specialist\": (1, 2),\n",
    "        \"Laboratory Technician\": (1, 2),\n",
    "        \"Materials Scientist (Metals)\": (1, 2),\n",
    "        \"Materials Scientist (Ceramics)\": (1, 2),\n",
    "        \"Materials Scientist (Composites)\": (1, 2),\n",
    "        \"Advanced Imaging Specialist\": (1, 2),\n",
    "        \"Analytical Instrument Specialist\": (1, 2),\n",
    "        \"Quality Assurance Officer\": (1, 1),\n",
    "        \"Project Manager\": (1, 1)\n",
    "    }\n",
    "    \n",
    "    sample_ranges = {\n",
    "        \"metals\": (5, 20),\n",
    "        \"ceramics\": (0, 20),\n",
    "        \"composites\": (0, 20),\n",
    "        \"polymers\": (0, 20)\n",
    "    }\n",
    "\n",
    "    # step_sizes = None\n",
    " \n",
    "    step_sizes = {\n",
    "        \"metals\": 10,\n",
    "        \"ceramics\": 10,\n",
    "        \"composites\": 10,\n",
    "        \"polymers\": 10,\n",
    "        **{r: 1 for r in resource_ranges.keys()}\n",
    "    }\n",
    "\n",
    "    resource_unit_costs = {\n",
    "        \"Administrative Support Specialist\": 75,\n",
    "        \"Laboratory Technician\": 100,\n",
    "        \"Materials Scientist (Metals)\": 150,\n",
    "        \"Materials Scientist (Ceramics)\": 150,\n",
    "        \"Materials Scientist (Composites)\": 150,\n",
    "        \"Advanced Imaging Specialist\": 150,\n",
    "        \"Analytical Instrument Specialist\": 125,\n",
    "        \"Quality Assurance Officer\": 125,\n",
    "        \"Project Manager\": 175\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"scenario_results_{timestamp}.csv\"\n",
    "        \n",
    "        results = run_capacity_analysis(\n",
    "            templates=templates,\n",
    "            resource_ranges=resource_ranges,\n",
    "            sample_ranges=sample_ranges,\n",
    "            resource_unit_costs=resource_unit_costs,\n",
    "            step_sizes=step_sizes,\n",
    "            output_file=output_file\n",
    "        )\n",
    "        \n",
    "        # Analyze and visualize results\n",
    "        results_df, fig = analyze_results(output_file)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Analysis failed: {str(e)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-21 21:38:19,213 - INFO - Starting capacity analysis\n",
      "Generated 6912 scenarios to evaluate\n",
      "2025-01-21 21:38:19,254 - INFO - \n",
      "Processing scenario 1/6912\n",
      "2025-01-21 21:38:19,254 - INFO - \n",
      "Processing scenario 2/6912\n",
      "2025-01-21 21:38:19,254 - INFO - Scenario configuration:\n",
      "2025-01-21 21:38:19,254 - INFO - \n",
      "Processing scenario 3/6912\n",
      "2025-01-21 21:38:19,264 - INFO - \n",
      "Processing scenario 4/6912\n",
      "2025-01-21 21:38:19,406 - INFO - Scenario configuration:\n",
      "2025-01-21 21:38:19,469 - INFO - Sample counts: Metals=5, Ceramics=0, Composites=0, Polymers=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenarios:   0%|          | 0/6912 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-21 21:38:19,477 - INFO - Scenario configuration:\n",
      "2025-01-21 21:38:19,480 - INFO - Scenario configuration:\n",
      "2025-01-21 21:38:19,480 - INFO - Sample counts: Metals=5, Ceramics=0, Composites=0, Polymers=10\n",
      "2025-01-21 21:38:19,488 - INFO - Resource capacities: {'Administrative Support Specialist': 1, 'Laboratory Technician': 1, 'Materials Scientist (Metals)': 1, 'Materials Scientist (Ceramics)': 1, 'Materials Scientist (Composites)': 1, 'Advanced Imaging Specialist': 1, 'Analytical Instrument Specialist': 1, 'Quality Assurance Officer': 1, 'Project Manager': 1}\n",
      "2025-01-21 21:38:19,490 - INFO - Sample counts: Metals=5, Ceramics=0, Composites=0, Polymers=20\n",
      "2025-01-21 21:38:19,498 - INFO - Sample counts: Metals=5, Ceramics=0, Composites=10, Polymers=0\n",
      "2025-01-21 21:38:19,498 - INFO - Resource capacities: {'Administrative Support Specialist': 1, 'Laboratory Technician': 1, 'Materials Scientist (Metals)': 1, 'Materials Scientist (Ceramics)': 1, 'Materials Scientist (Composites)': 1, 'Advanced Imaging Specialist': 1, 'Analytical Instrument Specialist': 1, 'Quality Assurance Officer': 1, 'Project Manager': 1}\n",
      "2025-01-21 21:38:19,500 - INFO - Generated 175 tasks for MET template\n",
      "2025-01-21 21:38:19,500 - INFO - Resource capacities: {'Administrative Support Specialist': 1, 'Laboratory Technician': 1, 'Materials Scientist (Metals)': 1, 'Materials Scientist (Ceramics)': 1, 'Materials Scientist (Composites)': 1, 'Advanced Imaging Specialist': 1, 'Analytical Instrument Specialist': 1, 'Quality Assurance Officer': 1, 'Project Manager': 1}\n",
      "2025-01-21 21:38:19,500 - INFO - Resource capacities: {'Administrative Support Specialist': 1, 'Laboratory Technician': 1, 'Materials Scientist (Metals)': 1, 'Materials Scientist (Ceramics)': 1, 'Materials Scientist (Composites)': 1, 'Advanced Imaging Specialist': 1, 'Analytical Instrument Specialist': 1, 'Quality Assurance Officer': 1, 'Project Manager': 1}\n",
      "2025-01-21 21:38:19,500 - INFO - Generated 175 tasks for MET template\n",
      "2025-01-21 21:38:19,500 - INFO - Total tasks generated: 175\n",
      "2025-01-21 21:38:19,508 - INFO - Generated 175 tasks for MET template\n",
      "2025-01-21 21:38:19,510 - INFO - Generated 175 tasks for MET template\n",
      "2025-01-21 21:38:19,510 - INFO - Generated 330 tasks for POLY template\n",
      "2025-01-21 21:38:19,510 - INFO - Tasks per resource:\n",
      "2025-01-21 21:38:19,510 - INFO - Generated 660 tasks for POLY template\n",
      "2025-01-21 21:38:19,510 - INFO - Generated 270 tasks for COMP template\n",
      "2025-01-21 21:38:19,518 - INFO - Total tasks generated: 505\n",
      "2025-01-21 21:38:19,518 - INFO -   Administrative Support Specialist: 30 tasks\n",
      "2025-01-21 21:38:19,520 - INFO - Total tasks generated: 835\n",
      "2025-01-21 21:38:19,520 - INFO - Total tasks generated: 445\n",
      "2025-01-21 21:38:19,520 - INFO - Tasks per resource:\n",
      "2025-01-21 21:38:19,520 - INFO -   Laboratory Technician: 50 tasks\n",
      "2025-01-21 21:38:19,520 - INFO - Tasks per resource:\n",
      "2025-01-21 21:38:19,520 - INFO - Tasks per resource:\n",
      "2025-01-21 21:38:19,528 - INFO -   Administrative Support Specialist: 90 tasks\n",
      "2025-01-21 21:38:19,530 - INFO -   Materials Scientist (Metals): 45 tasks\n",
      "2025-01-21 21:38:19,530 - INFO -   Administrative Support Specialist: 150 tasks\n",
      "2025-01-21 21:38:19,530 - INFO -   Administrative Support Specialist: 90 tasks\n",
      "2025-01-21 21:38:19,530 - INFO -   Laboratory Technician: 120 tasks\n",
      "2025-01-21 21:38:19,530 - INFO -   Analytical Instrument Specialist: 20 tasks\n",
      "2025-01-21 21:38:19,530 - INFO -   Laboratory Technician: 190 tasks\n",
      "2025-01-21 21:38:19,530 - INFO -   Laboratory Technician: 80 tasks\n",
      "2025-01-21 21:38:19,539 - INFO -   Materials Scientist (Metals): 45 tasks\n",
      "2025-01-21 21:38:19,539 - INFO -   Advanced Imaging Specialist: 15 tasks\n",
      "2025-01-21 21:38:19,541 - INFO -   Materials Scientist (Metals): 45 tasks\n",
      "2025-01-21 21:38:19,541 - INFO -   Materials Scientist (Metals): 45 tasks\n",
      "2025-01-21 21:38:19,541 - INFO -   Analytical Instrument Specialist: 50 tasks\n",
      "2025-01-21 21:38:19,541 - INFO -   Quality Assurance Officer: 5 tasks\n",
      "2025-01-21 21:38:19,541 - INFO -   Analytical Instrument Specialist: 80 tasks\n",
      "2025-01-21 21:38:19,541 - INFO -   Analytical Instrument Specialist: 30 tasks\n",
      "2025-01-21 21:38:19,549 - INFO -   Advanced Imaging Specialist: 35 tasks\n",
      "2025-01-21 21:38:19,551 - INFO -   Project Manager: 10 tasks\n",
      "2025-01-21 21:38:19,551 - INFO -   Advanced Imaging Specialist: 55 tasks\n",
      "2025-01-21 21:38:19,551 - INFO -   Advanced Imaging Specialist: 55 tasks\n",
      "2025-01-21 21:38:19,551 - INFO -   Quality Assurance Officer: 15 tasks\n",
      "2025-01-21 21:38:19,551 - INFO - Starting scheduling optimization...\n",
      "2025-01-21 21:38:19,551 - INFO -   Quality Assurance Officer: 25 tasks\n",
      "2025-01-21 21:38:19,559 - INFO -   Quality Assurance Officer: 15 tasks\n",
      "2025-01-21 21:38:19,559 - INFO -   Project Manager: 30 tasks\n",
      "2025-01-21 21:38:19,817 - INFO -   Project Manager: 50 tasks\n",
      "2025-01-21 21:38:20,010 - INFO -   Project Manager: 30 tasks\n",
      "2025-01-21 21:38:20,175 - INFO -   Materials Scientist (Composites): 120 tasks\n",
      "2025-01-21 21:38:20,236 - INFO -   Materials Scientist (Composites): 240 tasks\n",
      "2025-01-21 21:38:20,390 - INFO -   Materials Scientist (Composites): 100 tasks\n",
      "2025-01-21 21:38:20,551 - INFO - Starting scheduling optimization...\n",
      "2025-01-21 21:38:20,623 - INFO - Starting scheduling optimization...\n",
      "2025-01-21 21:38:20,715 - INFO - Starting scheduling optimization...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b86a16999e09dec7"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Create visualizations\n",
    "# Basic performance metrics\n",
    "fig_performance = visualize_results(results_df)\n",
    "plt.show()\n",
    "\n",
    "# Tradeoff analysis\n",
    "fig_tradeoffs = analyze_tradeoffs(results_df)\n",
    "plt.show()\n",
    "\n",
    "# Pareto frontier analysis\n",
    "fig_pareto = create_pareto_frontier(results_df)\n",
    "plt.show()"
   ],
   "id": "7db04cd0cd2a7f34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. Analyze specific scenarios in detail\n",
    "# Get the most efficient scenario\n",
    "efficient_scenario = results_df.sort_values('cost_per_sample').iloc[0]\n",
    "print(\"\\nMost Cost-Efficient Scenario:\")\n",
    "for col in results_df.columns:\n",
    "    if not col.startswith('utilization'):\n",
    "        print(f\"{col}: {efficient_scenario[col]}\")\n",
    "\n",
    "# Get highest throughput scenario\n",
    "max_throughput = results_df.sort_values('samples_per_year', ascending=False).iloc[0]\n",
    "print(\"\\nHighest Throughput Scenario:\")\n",
    "for col in results_df.columns:\n",
    "    if not col.startswith('utilization'):\n",
    "        print(f\"{col}: {max_throughput[col]}\")"
   ],
   "id": "599887b1d413ebd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Create Gantt chart for a specific scenario\n",
    "# First, run the scheduler for the scenario we want to visualize\n",
    "scenario_tasks = []\n",
    "for template, count, prefix in zip(\n",
    "    [metals_template, ceramics_template, composites_template, polymers_template],\n",
    "    [efficient_scenario['samples_metals'], efficient_scenario['samples_ceramics'], \n",
    "     efficient_scenario['samples_composites'], efficient_scenario['samples_polymers']],\n",
    "    [\"MET_\", \"CER_\", \"COMP_\", \"POLY_\"]\n",
    "):\n",
    "    if count > 0:\n",
    "        scenario_tasks.extend(replicate_wbs_optimized(template, count, prefix))\n",
    "\n",
    "# Get resource capacities from the scenario\n",
    "resource_capacities = {\n",
    "    resource: efficient_scenario[f'resource_{resource}']\n",
    "    for resource in resource_ranges.keys()\n",
    "}\n",
    "\n",
    "# Run scheduler\n",
    "makespan, start_times = schedule_optimized(\n",
    "    tasks=scenario_tasks,\n",
    "    resource_capacities=resource_capacities\n",
    ")\n",
    "\n",
    "# Create Gantt chart\n",
    "if makespan and start_times:\n",
    "    fig_gantt = create_gantt_chart(\n",
    "        tasks=scenario_tasks,\n",
    "        start_times=start_times,\n",
    "        resource_capacities=resource_capacities,\n",
    "        makespan=makespan\n",
    "    )\n",
    "    plt.show()"
   ],
   "id": "d0cdde8a6b7b3404",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. Generate summary report\n",
    "summary = f\"\"\"\n",
    "Analysis Summary ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n",
    "================================================\n",
    "\n",
    "Dataset Overview:\n",
    "----------------\n",
    "Total Scenarios Analyzed: {len(results_df)}\n",
    "Feasible Scenarios: {len(results_df[results_df['makespan'].notna()])}\n",
    "Resource Configurations: {len(results_df.groupby([col for col in results_df.columns if col.startswith('resource_')]))}\n",
    "\n",
    "Performance Metrics:\n",
    "------------------\n",
    "Average Throughput: {results_df['samples_per_year'].mean():.1f} samples/year\n",
    "Average Cost per Sample: ${results_df['cost_per_sample'].mean():.2f}\n",
    "Average Makespan: {results_df['makespan'].mean():.1f} days\n",
    "\n",
    "Most Efficient Configuration:\n",
    "---------------------------\n",
    "Cost per Sample: ${efficient_scenario['cost_per_sample']:.2f}\n",
    "Annual Throughput: {efficient_scenario['samples_per_year']:.0f}\n",
    "Total Cost: ${efficient_scenario['total_cost']:.2f}\n",
    "\n",
    "Resource Utilization:\n",
    "-------------------\n",
    "{results_df[[col for col in results_df.columns if col.startswith('utilization_')]].mean().to_string()}\n",
    "\n",
    "Visualization Files:\n",
    "------------------\n",
    "- Performance Metrics: performance_metrics_{timestamp}.png\n",
    "- Tradeoff Analysis: tradeoff_analysis_{timestamp}.png\n",
    "- Pareto Frontier: pareto_frontier_{timestamp}.png\n",
    "- Gantt Chart: gantt_chart_{timestamp}.png\n",
    "\"\"\"\n",
    "\n",
    "with open(f'analysis_summary_{timestamp}.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nAnalysis complete! Check the output files for detailed results.\")"
   ],
   "id": "ace4bfc6df25beae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
